max_seq_len: 2048
seed: 1
# precision: amp_bf16
precision: fp32 

# models:
# -
#   model_name: mpt-7b-8k
#   model:
#     name: hf_causal_lm
#     pretrained_model_name_or_path: mosaicml/mpt-7b-8k
#     init_device: mixed
#     pretrained: true
#     config_overrides:
#       max_seq_len: ${max_seq_len}
#   tokenizer:
#     name: mosaicml/mpt-7b
#     kwargs:
#       model_max_length: ${max_seq_len}

# If you are using one model, put it here:
model_name_or_path: EleutherAI/gpt-neo-125m
# otherwise, write a block for each model you want to test in the `models` section

models:
-
  model_name: ${model_name_or_path}
  model:
    name: hf_causal_lm
    pretrained_model_name_or_path: ${model_name_or_path}
    init_device: cpu 
    pretrained: true
  tokenizer:
    name: ${model_name_or_path}
    kwargs:
      model_max_length: ${max_seq_len}

device_eval_batch_size: 1

# FSDP config for model sharding
fsdp_config:
  sharding_strategy: FULL_SHARD
  mixed_precision: FULL
  forward_prefetch: True
  limit_all_gathers: True

icl_tasks:
# -
#   label: arc_challenge
#   dataset_uri: eval/local_data/world_knowledge/arc_challenge.jsonl
#   num_fewshot: [5]
#   icl_task_type: multiple_choice
#   continuation_delimiter: "\nAnswer: "
#   icl_subset_num_batches: 10
# -
#   label: triviaqa_sm_sub
#   dataset_uri: eval/local_data/world_knowledge/triviaqa_sm_sub.jsonl
#   num_fewshot: [3]
#   icl_task_type: question_answering
-
  label: ifeval
  dataset_uri: eval/local_data/ift/ifeval_small.jsonl
  num_fewshot: [0]
  icl_task_type: ifeval 

eval_gauntlet:
  weighting: EQUAL
  subtract_random_baseline: true
  rescale_accuracy: true
  averages:
    core_average:
    - ift
  categories:
  - name: ift
    benchmarks:
    - name: ifeval
      num_fewshot: 0
      random_baseline: 0